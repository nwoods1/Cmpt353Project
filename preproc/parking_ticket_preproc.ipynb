{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parking Tickets Pre-Processing\n",
    "\n",
    "Extract the data from the large csv file into smaller, more memory-efficient compressed csv.gz format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/04/04 05:10:48 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/04/04 05:10:48 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession, types\n",
    "\n",
    "spark = SparkSession.builder.appName(\"parking-ticket-preproc\").getOrCreate()\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "\n",
    "assert sys.version_info >= (3, 5)  # make sure we have Python 3.5+\n",
    "assert spark.version >= \"2.4\"  # make sure we have Spark 2.4+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# schema of the raw csv file\n",
    "schema = types.StructType(\n",
    "    [\n",
    "        types.StructField(\"block\", types.IntegerType()),\n",
    "        types.StructField(\"street\", types.StringType()),\n",
    "        types.StructField(\"entrydate\", types.DateType()),\n",
    "        types.StructField(\"bylaw\", types.IntegerType()),\n",
    "        types.StructField(\"section\", types.StringType()),\n",
    "        types.StructField(\"status\", types.StringType()),\n",
    "        types.StructField(\"infractiontext\", types.StringType()),\n",
    "        types.StructField(\"year\", types.StringType()),\n",
    "        types.StructField(\"bi_id\", types.IntegerType())\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants\n",
    "in_file = \"parking-tickets.csv\"\n",
    "num_partitions = 10\n",
    "out_dir = \"../parking-tickets\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# read the csv\n",
    "raw_df = spark.read.csv(in_file, schema=schema, sep=\";\")\n",
    "raw_df = raw_df.na.drop()\n",
    "# partition the csv\n",
    "original_df = raw_df.repartition(num_partitions)\n",
    "\n",
    "# store the partitioned data\n",
    "original_df.write.csv(out_dir, compression=\"gzip\", mode=\"overwrite\", header=True, sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove extra files and rename gz files\n",
    "import os\n",
    "i = 0\n",
    "for root, dirs, files in os.walk(out_dir):\n",
    "    for file in files:\n",
    "        file_path = os.path.join(root, file)\n",
    "        if file == \"_SUCCESS\" or file.endswith(\".crc\"):\n",
    "            os.remove(file_path)\n",
    "        elif file.endswith(\".gz\"):\n",
    "            os.rename(file_path, f\"{out_dir}/parking-ticket-{i}.json.gz\")\n",
    "            i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation\n",
    "\n",
    "Ensure no data was lost during the partition and compression process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected 10 json.gz files for parking tickets\n",
      "File: ../parking-tickets/parking-ticket-0.json.gz, Size: 1504.56 KB\n",
      "File: ../parking-tickets/parking-ticket-1.json.gz, Size: 1505.85 KB\n",
      "File: ../parking-tickets/parking-ticket-2.json.gz, Size: 1504.48 KB\n",
      "File: ../parking-tickets/parking-ticket-3.json.gz, Size: 1502.92 KB\n",
      "File: ../parking-tickets/parking-ticket-4.json.gz, Size: 1504.64 KB\n",
      "File: ../parking-tickets/parking-ticket-5.json.gz, Size: 1508.65 KB\n",
      "File: ../parking-tickets/parking-ticket-6.json.gz, Size: 1502.42 KB\n",
      "File: ../parking-tickets/parking-ticket-7.json.gz, Size: 1505.38 KB\n",
      "File: ../parking-tickets/parking-ticket-8.json.gz, Size: 1501.93 KB\n",
      "File: ../parking-tickets/parking-ticket-9.json.gz, Size: 1503.56 KB\n"
     ]
    }
   ],
   "source": [
    "print(f\"Expected {num_partitions} json.gz files for parking tickets\")\n",
    "\n",
    "for root, dirs, files in os.walk(out_dir):\n",
    "    for file in files:\n",
    "        if file.endswith(\".gz\"):  # Check for gzip compressed files\n",
    "            file_path = os.path.join(root, file)\n",
    "            file_size = os.path.getsize(file_path)  # Get the size of the file in bytes\n",
    "            print(f\"File: {file_path}, Size: {file_size / 1024:.2f} KB\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/04 05:11:01 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/04/04 05:11:01 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/04/04 05:11:01 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/04/04 05:11:01 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/04/04 05:11:01 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/04/04 05:11:01 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/04/04 05:11:01 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/04/04 05:11:01 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/04/04 05:11:01 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/04/04 05:11:01 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/04/04 05:11:02 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/04/04 05:11:02 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/04/04 05:11:02 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/04/04 05:11:02 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/04/04 05:11:02 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/04/04 05:11:02 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/04/04 05:11:02 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/04/04 05:11:02 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/04/04 05:11:02 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/04/04 05:11:02 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/04/04 05:11:06 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/04/04 05:11:06 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/04/04 05:11:06 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/04/04 05:11:06 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/04/04 05:11:06 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/04/04 05:11:06 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/04/04 05:11:06 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/04/04 05:11:06 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/04/04 05:11:06 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/04/04 05:11:06 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/04/04 05:11:10 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/04/04 05:11:11 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/04/04 05:11:11 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/04/04 05:11:11 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/04/04 05:11:11 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/04/04 05:11:11 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/04/04 05:11:11 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/04/04 05:11:11 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/04/04 05:11:11 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/04/04 05:11:11 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/04/04 05:11:14 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/04/04 05:11:14 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/04/04 05:11:14 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/04/04 05:11:14 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/04/04 05:11:14 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/04/04 05:11:14 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/04/04 05:11:14 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/04/04 05:11:14 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/04/04 05:11:14 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/04/04 05:11:14 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "[Stage 44:==================================================>     (18 + 2) / 20]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrames are equal!\n",
      "Split and compression successful: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# compare the raw and compressed dataset for equality\n",
    "\n",
    "def compare_dfs(df1, df2):\n",
    "# Compare schemas\n",
    "    if df1.schema != df2.schema:\n",
    "        print(\"Schemas are different!\")\n",
    "        print(f\"df1 schema: {df1.schema}\")\n",
    "        print(f\"df2 schema: {df2.schema}\")\n",
    "        return False\n",
    "\n",
    "    # Compare data\n",
    "    if df1.count() != df2.count():\n",
    "        print(f\"Row counts are different: {df1.count()} | {df2.count()}\")\n",
    "        return False\n",
    "\n",
    "    diff1 = df1.exceptAll(df2)\n",
    "    diff2 = df2.exceptAll(df1)\n",
    "\n",
    "    # Print out the rows that are different\n",
    "    if diff1.count() > 0:\n",
    "        print(\"Rows in df1 but not in df2:\")\n",
    "        diff1.show(truncate=False)\n",
    "\n",
    "    if diff2.count() > 0:\n",
    "        print(\"Rows in df2 but not in df1:\")\n",
    "        diff2.show(truncate=False)\n",
    "\n",
    "    if diff1.count() == 0 and diff2.count() == 0:\n",
    "        print(\"DataFrames are equal!\")\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "compressed_df = spark.read.csv(out_dir, schema=schema, header=True, sep=\";\")\n",
    "print(f\"Split and compression successful: {compare_dfs(original_df, compressed_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------------+----------+-----+-----------+------+--------------------+----+-------+\n",
      "|block|          street| entrydate|bylaw|    section|status|      infractiontext|year|  bi_id|\n",
      "+-----+----------------+----------+-----+-----------+------+--------------------+----+-------+\n",
      "|  100|       W 8TH AVE|2024-10-19| 2952|    5(4)(B)|    IS|PARK IN A METERED...|2024|4547115|\n",
      "| 1100|      ALBERNI ST|2024-11-02| 2952|    5(4)(B)|    IS|PARK IN A METERED...|2024|4555064|\n",
      "| 1100|      SEYMOUR ST|2024-12-29| 2952|5(4)(A)(ii)|    IS|PARK IN A METERED...|2024|4716613|\n",
      "|  100|       W 3RD AVE|2023-05-07| 2952|5(4)(A)(ii)|    IS|PARK IN A METERED...|2023|4557889|\n",
      "| 1700|      E 11TH AVE|2023-07-21| 2849|    17.6(B)|    IS|PARK ON A STREET ...|2023|4563078|\n",
      "|  200|       ROBSON ST|2024-11-29| 2952|    5(4)(B)|    IS|PARK IN A METERED...|2024|4701248|\n",
      "| 2400|          ASH ST|2024-11-04| 2952|5(4)(A)(ii)|    IS|PARK IN A METERED...|2024|4696759|\n",
      "|  800|        DRAKE ST|2023-03-10| 2952|5(4)(A)(ii)|    IS|PARK IN A METERED...|2023|4510752|\n",
      "| 2700|      W BROADWAY|2023-09-05| 2952|5(4)(A)(ii)|    IS|PARK IN A METERED...|2023|4492517|\n",
      "| 1000|COMMERCIAL DRIVE|2022-12-27| 2952|5(4)(A)(ii)|    IS|PARK IN A METERED...|2022|4470073|\n",
      "|  800|     RICHARDS ST|2023-08-16| 2849|    17.6(B)|    VA|PARK ON A STREET ...|2023|4552047|\n",
      "| 1700|      ONTARIO ST|2024-10-27| 2952|    5(4)(B)|    IS|PARK IN A METERED...|2024|4677338|\n",
      "|  700|         BUTE ST|2023-03-12| 2952|5(4)(A)(ii)|    IS|PARK IN A METERED...|2023|4522541|\n",
      "| 3300|         MAIN ST|2024-11-29| 2952|    5(4)(B)|    IS|PARK IN A METERED...|2024|4701074|\n",
      "| 3600|      E 28TH AVE|2024-10-18| 2849|     72A(1)|    VA|NO PERSON SHALL P...|2024|4692444|\n",
      "|  300|   E HASTINGS ST|2024-10-18| 2849|    19.1(H)|    IS|STOP ON EITHER SI...|2024|4692570|\n",
      "|  200|     HEATLEY AVE|2022-12-12| 2849|    17.1(B)|    IS|STOP AT A PLACE W...|2022|4448665|\n",
      "| 2200|      W 41ST AVE|2023-10-12| 2849|    17.1(B)|    IS|STOP AT A PLACE W...|2023|4544575|\n",
      "| 1000|      ALBERNI ST|2023-07-21| 2952|    5(4)(B)|    IS|PARK IN A METERED...|2023|4564706|\n",
      "| 4300|         MAIN ST|2024-11-28| 2952|    5(4)(B)|    IS|PARK IN A METERED...|2024|4705282|\n",
      "+-----+----------------+----------+-----+-----------+------+--------------------+----+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "compressed_df.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
